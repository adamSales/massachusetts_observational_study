{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "effective-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "threatened-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_csv('raw_data/raw_remnant_input.csv')\n",
    "target_data = pd.read_csv('raw_data/raw_remnant_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wireless-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['directory_1', \n",
    "                         'directory_2', \n",
    "                         'directory_3', \n",
    "                         'sequence_id', \n",
    "                         'is_skill_builder', \n",
    "                         'has_due_date', \n",
    "                         'assignment_completed']\n",
    "\n",
    "continuous_features = ['time_since_last_assignment_start', \n",
    "                        'session_count_raw',\n",
    "                        'session_count_normalized', \n",
    "                        'session_count_class_percentile',\n",
    "                        'day_count_raw', \n",
    "                        'day_count_normalized', \n",
    "                        'day_count_class_percentile',\n",
    "                        'completed_problem_count_raw', \n",
    "                        'completed_problem_count_normalized',\n",
    "                        'completed_problem_count_class_percentile',\n",
    "                        'median_ln_problem_time_on_task_raw',\n",
    "                        'median_ln_problem_time_on_task_normalized',\n",
    "                        'median_ln_problem_time_on_task_class_percentile',\n",
    "                        'median_ln_problem_first_response_time_raw',\n",
    "                        'median_ln_problem_first_response_time_normalized',\n",
    "                        'median_ln_problem_first_response_time_class_percentile',\n",
    "                        'average_problem_attempt_count',\n",
    "                        'average_problem_attempt_count_normalized',\n",
    "                        'average_problem_attempt_count_class_percentile',\n",
    "                        'average_problem_answer_first',\n",
    "                        'average_problem_answer_first_normalized',\n",
    "                        'average_problem_answer_first_class_percentile',\n",
    "                        'average_problem_correctness', \n",
    "                        'average_problem_correctness_normalized',\n",
    "                        'average_problem_correctness_class_percentile',\n",
    "                        'average_problem_hint_count', \n",
    "                        'average_problem_hint_count_normalized',\n",
    "                        'average_problem_hint_count_class_percentile',\n",
    "                        'average_problem_answer_given',\n",
    "                        'average_problem_answer_given_normalized',\n",
    "                        'average_problem_answer_given_class_percentile',\n",
    "                        'time_since_last_assignment_start_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-girlfriend",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the distributions of all the raw inputs, or the frequency of values in the raw inputs\n",
    "\n",
    "PLOT_ROWS = 14\n",
    "PLOT_COLS = 3\n",
    "\n",
    "fig, axs = plt.subplots(PLOT_ROWS, PLOT_COLS, figsize=(20,50))\n",
    "\n",
    "offset = PLOT_ROWS * PLOT_COLS - len(input_data.columns)\n",
    "\n",
    "for i in range(len(input_data.columns)):\n",
    "    r = int((i + offset) / PLOT_COLS)\n",
    "    c = (i + offset) % PLOT_COLS\n",
    "    col = input_data.columns[i]\n",
    "    axs[r, c].hist(input_data[col].value_counts() if i < 5 else input_data[col], 50)\n",
    "    axs[r, c].set_title(col + '_value_counts' if i < 5 else col)\n",
    "fig.savefig('first_look.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "retained-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features\n",
    "\n",
    "# An explicit feature for which cluster the time_since_last_assignment_start falls into\n",
    "clusters = 4\n",
    "times = input_data['time_since_last_assignment_start'].values.reshape(-1, 1)\n",
    "input_data['time_since_last_assignment_start_cluster'] = GaussianMixture(n_components=clusters).fit(times).predict(times)\n",
    "categorical_features.append('time_since_last_assignment_start_cluster')\n",
    "\n",
    "# A feature for whether or not there is a folder path\n",
    "input_data['custom_assignment'] = input_data['directory_1'].isna().astype(int)\n",
    "categorical_features.append('custom_assignment')\n",
    "\n",
    "# A feature for whether or not there is any problem level data\n",
    "input_data['no_problem_statsistics'] = input_data['median_ln_problem_time_on_task_raw'].isna().astype(int)\n",
    "categorical_features.append('no_problem_statsistics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "precise-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing categorical values and prepare to one-hot encode them\n",
    "\n",
    "input_data[categorical_features] = input_data[categorical_features].fillna(-1)\n",
    "\n",
    "one_hot_encoder = OneHotEncoder().fit(input_data[categorical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataframe into sub dataframes for each target\n",
    "# figure out the max length\n",
    "# kmeans with stratification on student from targets\n",
    "# normalize and one hot encode the training and test input\n",
    "# fill in the length of the sequence and the missing values with 0\n",
    "# train the model\n",
    "# test the model\n",
    "# average the results of each set in the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "brutal-length",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalize the continuous variables\n",
    "normalizer = StandardScaler().fit(input_data[continuous_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-violin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
