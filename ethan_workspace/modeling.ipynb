{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effective-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "threatened-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_csv('raw_data/raw_remnant_input.csv')\n",
    "modeling_data = pd.read_csv('raw_data/raw_remnant_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wireless-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['directory_1', \n",
    "                         'directory_2', \n",
    "                         'directory_3', \n",
    "                         'sequence_id', \n",
    "                         'is_skill_builder', \n",
    "                         'has_due_date', \n",
    "                         'assignment_completed']\n",
    "\n",
    "continuous_features = ['time_since_last_assignment_start', \n",
    "                        'session_count_raw',\n",
    "                        'session_count_normalized', \n",
    "                        'session_count_class_percentile',\n",
    "                        'day_count_raw', \n",
    "                        'day_count_normalized', \n",
    "                        'day_count_class_percentile',\n",
    "                        'completed_problem_count_raw', \n",
    "                        'completed_problem_count_normalized',\n",
    "                        'completed_problem_count_class_percentile',\n",
    "                        'median_ln_problem_time_on_task_raw',\n",
    "                        'median_ln_problem_time_on_task_normalized',\n",
    "                        'median_ln_problem_time_on_task_class_percentile',\n",
    "                        'median_ln_problem_first_response_time_raw',\n",
    "                        'median_ln_problem_first_response_time_normalized',\n",
    "                        'median_ln_problem_first_response_time_class_percentile',\n",
    "                        'average_problem_attempt_count',\n",
    "                        'average_problem_attempt_count_normalized',\n",
    "                        'average_problem_attempt_count_class_percentile',\n",
    "                        'average_problem_answer_first',\n",
    "                        'average_problem_answer_first_normalized',\n",
    "                        'average_problem_answer_first_class_percentile',\n",
    "                        'average_problem_correctness', \n",
    "                        'average_problem_correctness_normalized',\n",
    "                        'average_problem_correctness_class_percentile',\n",
    "                        'average_problem_hint_count', \n",
    "                        'average_problem_hint_count_normalized',\n",
    "                        'average_problem_hint_count_class_percentile',\n",
    "                        'average_problem_answer_given',\n",
    "                        'average_problem_answer_given_normalized',\n",
    "                        'average_problem_answer_given_class_percentile',\n",
    "                        'time_since_last_assignment_start_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-girlfriend",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the distributions of all the raw inputs, or the frequency of values in the raw inputs\n",
    "\n",
    "PLOT_ROWS = 14\n",
    "PLOT_COLS = 3\n",
    "\n",
    "fig, axs = plt.subplots(PLOT_ROWS, PLOT_COLS, figsize=(20,50))\n",
    "\n",
    "offset = PLOT_ROWS * PLOT_COLS - len(input_data.columns)\n",
    "\n",
    "for i in range(len(input_data.columns)):\n",
    "    r = int((i + offset) / PLOT_COLS)\n",
    "    c = (i + offset) % PLOT_COLS\n",
    "    col = input_data.columns[i]\n",
    "    axs[r, c].hist(input_data[col].value_counts() if i < 5 else input_data[col], 50)\n",
    "    axs[r, c].set_title(col + '_value_counts' if i < 5 else col)\n",
    "fig.savefig('first_look.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "retained-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features\n",
    "\n",
    "# An explicit feature for which cluster the time_since_last_assignment_start falls into\n",
    "clusters = 4\n",
    "times = input_data['time_since_last_assignment_start'].values.reshape(-1, 1)\n",
    "input_data['time_since_last_assignment_start_cluster'] = GaussianMixture(n_components=clusters).fit(times).predict(times)\n",
    "categorical_features.append('time_since_last_assignment_start_cluster')\n",
    "\n",
    "# A feature for whether or not there is a folder path\n",
    "input_data['custom_assignment'] = input_data['directory_1'].isna().astype(int)\n",
    "categorical_features.append('custom_assignment')\n",
    "\n",
    "# A feature for whether or not there is any problem level data\n",
    "input_data['no_problem_statsistics'] = input_data['median_ln_problem_time_on_task_raw'].isna().astype(int)\n",
    "categorical_features.append('no_problem_statsistics')\n",
    "\n",
    "# Replace NaN categorical features with -1\n",
    "input_data[categorical_features] = input_data[categorical_features].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "several-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the previous assignments to the training data\n",
    "modeling_data['previous_assignments'] = None\n",
    "\n",
    "for i, row in modeling_data.iterrows():\n",
    "    row_input = input_data[(input_data['assignment_start_time'] < row['assignment_start_time']) & (input_data['student_id'] == row['student_id'])].sort_values('assignment_start_time')\n",
    "    if len(row_input) > 0:\n",
    "        modeling_data.at[i, 'previous_assignments'] = row_input\n",
    "\n",
    "modeling_data = modeling_data[~modeling_data['previous_assignments'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "living-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auc maybe kappa and rmse, r2, naglekerke r2 for categorical\n",
    "\n",
    "def process_input(df_list, one_hot_encoder=None, normalizer=None, max_sequence_length=20):\n",
    "    \n",
    "    if one_hot_encoder is None:\n",
    "        categorical_data = np.concatenate([df[categorical_features].values for df in df_list])\n",
    "        continuous_data = np.concatenate([df[continuous_features].values for df in df_list])\n",
    "        one_hot_encoder = OneHotEncoder(handle_unknown='ignore').fit(categorical_data)\n",
    "        normalizer = StandardScaler().fit(continuous_data)\n",
    "    \n",
    "    processed_input = []\n",
    "    for df in df_list:\n",
    "        categorical_data = one_hot_encoder.transform(df[categorical_features]).toarray()\n",
    "        continuous_data = np.nan_to_num(normalizer.transform(df[continuous_features]))\n",
    "        combined_data = np.concatenate([categorical_data, continuous_data], axis=1)\n",
    "        if combined_data.shape[0] >= max_sequence_length:\n",
    "            resized_data = combined_data[-max_sequence_length:,:]\n",
    "        else:\n",
    "            resized_data = np.zeros((max_sequence_length, combined_data.shape[1]))\n",
    "            resized_data[-combined_data.shape[0]:,:] = combined_data\n",
    "        processed_input.append(resized_data)\n",
    "    \n",
    "    return np.stack(processed_input), one_hot_encoder, normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "characteristic-extent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4400, 20, 1633)\n",
      "(4400, 2)\n",
      "(1101, 20, 1633)\n",
      "(1101, 2)\n",
      "(4401, 20, 1669)\n",
      "(4401, 2)\n",
      "(1100, 20, 1669)\n",
      "(1100, 2)\n",
      "(4401, 20, 1663)\n",
      "(4401, 2)\n",
      "(1100, 20, 1663)\n",
      "(1100, 2)\n",
      "(4401, 20, 1663)\n",
      "(4401, 2)\n",
      "(1100, 20, 1663)\n",
      "(1100, 2)\n",
      "(4401, 20, 1674)\n",
      "(4401, 2)\n",
      "(1100, 20, 1674)\n",
      "(1100, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "# Normalize and one hot encode the data based on the input data\n",
    "\n",
    "N_SPLITS = 5\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "for train_index, test_index in GroupKFold(N_SPLITS).split(modeling_data, groups=modeling_data['student_id']):\n",
    "    training_input, input_one_hot_encoder, input_normalizer = process_input(modeling_data.iloc[train_index]['previous_assignments'].tolist())\n",
    "    print(training_input.shape)\n",
    "    training_target = modeling_data.iloc[train_index][['assignment_completed', 'problems_completed']].values\n",
    "    print(training_target.shape)\n",
    "    testing_input, _, _ = process_input(modeling_data.iloc[test_index]['previous_assignments'].tolist(), input_one_hot_encoder, input_normalizer)\n",
    "    print(testing_input.shape)\n",
    "    testing_target = modeling_data.iloc[test_index][['assignment_completed', 'problems_completed']].values\n",
    "    print(testing_target.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans with stratification on student from targets\n",
    "# normalize and one hot encode the training and test input\n",
    "# fill in the length of the sequence and the missing values with 0\n",
    "# train the model\n",
    "# test the model\n",
    "# average the results of each set in the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-length",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalize the continuous variables\n",
    "normalizer = StandardScaler().fit(input_data[continuous_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-violin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
