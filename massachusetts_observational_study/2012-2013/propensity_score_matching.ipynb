{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Masking, LSTM, Dense, Dropout, Concatenate\n",
    "\n",
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine treatment\n",
    "assistments_usage = pd.read_csv('assistments_data/csv/school_year_data.csv', dtype=str).astype(str)\n",
    "assistments_usage = assistments_usage.set_index('school_id')\n",
    "assistments_usage = assistments_usage.astype(float)\n",
    "\n",
    "mass_doe_data = pd.read_csv('mcas_exports/csv/mass_doe_data.csv', dtype=str).astype(str)\n",
    "mass_doe_data = mass_doe_data.set_index('school_code')\n",
    "float_columns = [c for c in mass_doe_data if c != 'prior_performance']\n",
    "mass_doe_data[float_columns] = mass_doe_data[float_columns].astype(float)\n",
    "mass_doe_data['prior_performance'] = mass_doe_data['prior_performance'].apply(lambda x: np.array(eval(x.replace('nan', 'np.nan'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment = assistments_usage[assistments_usage['school_end_year'].isin([2012, 2013])]\n",
    "treatment = treatment.groupby('school_id')['assignment_log_count'].mean()\n",
    "treatment = treatment.rename('treatment_usage')\n",
    "\n",
    "pretreatment = assistments_usage[assistments_usage['school_end_year'] < 2012]\n",
    "pretreatment = pretreatment.groupby('school_id')['assignment_log_count'].mean()\n",
    "pretreatment = pretreatment.rename('pretreatment_usage')\n",
    "\n",
    "population = mass_doe_data['6'] + mass_doe_data['7'] + mass_doe_data['8']\n",
    "population = population.rename('population')\n",
    "\n",
    "participation = pd.concat([treatment, pretreatment, population], axis=1, sort=True)\n",
    "participation = participation.dropna(subset=['treatment_usage', 'population'])\n",
    "participation = participation.fillna(0)\n",
    "treatment_check = (participation['treatment_usage'] / participation['population'] >= 1)\n",
    "pretreatment_check = (participation['pretreatment_usage'] / participation['population'] < 0.5)\n",
    "participants = participation.index[treatment_check & pretreatment_check]\n",
    "\n",
    "mass_doe_data['in_treatment'] = mass_doe_data.index.isin(participants).astype(int)\n",
    "mass_doe_data['in_remnant'] = 1 - mass_doe_data.index.isin(assistments_usage.index).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get model stats and propensity scores for every school\n",
    "\n",
    "propensity_scores = []\n",
    "aucs = []\n",
    "r2s = []\n",
    "for train_index, test_index in StratifiedKFold(n_splits=5, shuffle=True, random_state=0).split(ff_X, y):\n",
    "    \n",
    "    # Clear session so models don't pile up\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    # Split data into training and testing splits\n",
    "    train_ff_X, test_ff_X = ff_X[train_index], ff_X[test_index]\n",
    "    train_lstm_X, test_lstm_X = lstm_X[train_index], lstm_X[test_index]\n",
    "    train_y, test_y = y[train_index], y[test_index]\n",
    "    train_w, test_w = w[train_index], w[test_index]\n",
    "    \n",
    "    # Normalize the input data based on the training data distribution\n",
    "    ff_scaler = StandardScaler().fit(train_ff_X)\n",
    "    train_ff_X = np.nan_to_num(ff_scaler.transform(train_ff_X))\n",
    "    test_ff_X = np.nan_to_num(ff_scaler.transform(test_ff_X))\n",
    "    \n",
    "    train_lstm_X_shape = train_lstm_X.shape\n",
    "    train_stacked_lstm_X = train_lstm_X.reshape(-1, train_lstm_X_shape[-1])\n",
    "    lstm_scaler = StandardScaler().fit(train_stacked_lstm_X)\n",
    "    train_lstm_X = np.nan_to_num(lstm_scaler.transform(train_stacked_lstm_X)).reshape(train_lstm_X_shape)\n",
    "    test_lstm_X_shape = test_lstm_X.shape\n",
    "    test_stacked_lstm_X = test_lstm_X.reshape(-1, test_lstm_X_shape[-1])\n",
    "    test_lstm_X = np.nan_to_num(lstm_scaler.transform(test_stacked_lstm_X)).reshape(test_lstm_X_shape)\n",
    "    \n",
    "    # Create the neural network\n",
    "    ff_input_layer = Input(shape=train_ff_X[0].shape)\n",
    "    ff_model = Dense(units=64, activation='tanh')(ff_input_layer)\n",
    "    ff_model = Dropout(rate=0.5)(ff_model)\n",
    "    \n",
    "    lstm_input_layer = Input(shape=train_lstm_X[0].shape)\n",
    "    lstm_model = Masking(mask_value=0.0)(lstm_input_layer)\n",
    "    lstm_model = LSTM(units=64, return_sequences=False, activation='tanh', dropout=0.5, recurrent_dropout=0.5)(lstm_model)\n",
    "    \n",
    "    #model = Concatenate()([ff_model, lstm_model])\n",
    "    model = ff_model\n",
    "    output_layer = Dense(units=1, activation='sigmoid')(model)\n",
    "    \n",
    "    #combined_model = Model([ff_input_layer, lstm_input_layer], output_layer)\n",
    "    combined_model = Model(ff_input_layer, output_layer)\n",
    "    combined_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    \n",
    "    # Train the neural network\n",
    "    es = [EarlyStopping(monitor='val_loss', patience=10, min_delta=0, restore_best_weights=True)]\n",
    "    combined_model.fit(x=train_ff_X, #x=[train_ff_X, train_lstm_X],\n",
    "                       y=train_y,\n",
    "                       epochs=1000,\n",
    "                       validation_split=0.25,\n",
    "                       callbacks=es,\n",
    "                       sample_weight=train_w,\n",
    "                       verbose=0)\n",
    "    \n",
    "    # Use the neural network to predict the held-out fold\n",
    "    #pred_y = combined_model.predict([test_ff_X, test_lstm_X]).flatten()\n",
    "    pred_y = combined_model.predict(test_ff_X).flatten()\n",
    "    \n",
    "    # Update propensity scores and metrics\n",
    "    propensity_scores.extend(pred_y.tolist())\n",
    "    aucs.append(roc_auc_score(test_y.flatten(), pred_y))\n",
    "    r2s.append(r2_score(test_y.flatten(), pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Findings\n",
    "\n",
    "print(f'Model AUC = {np.mean(aucs):.4g}')\n",
    "print(f'Model R\\u00b2 = {np.mean(r2s):.4g}')\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(propensity_scores, bins=20)\n",
    "plt.xlim([0,1])\n",
    "plt.xlabel('Propensity Score')\n",
    "plt.ylabel('Prediction Frequency')\n",
    "plt.title('Frequency of Propensity Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model AUC = 0.5519\n",
    "Model RÂ² = -10.7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
