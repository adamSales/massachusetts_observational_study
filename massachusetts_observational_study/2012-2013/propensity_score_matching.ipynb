{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Masking, LSTM, Dense, Dropout, Concatenate\n",
    "\n",
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# Constants\n",
    "SCRIPT_DIR = 'assistments_data/sql'\n",
    "OUT_DIR = 'assistments_data/csv'\n",
    "TMP_DIR = 'assistments_data/sql/tmp'\n",
    "DATA_SCRIPT = 'year_data_query.sql'\n",
    "TMP_SCRIPT = 'tmp.sql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sql instance\n",
    "manager = rds.Manager()\n",
    "credentials = manager.get_db_credentials('rds-dev', 'tng')\n",
    "url = f'postgresql://{credentials.username}:{credentials.password}@{credentials.host}:{credentials.port}/cas_core'\n",
    "dev_engine = create_engine(url)\n",
    "dev_conn = dev_engine.connect().execution_options(autocommit=True)\n",
    "\n",
    "# Get school year data\n",
    "school_year_data = []\n",
    "for year in [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]:\n",
    "    if os.path.exists(TMP_DIR):\n",
    "        shutil.rmtree(TMP_DIR)\n",
    "    os.makedirs(TMP_DIR)\n",
    "    with open(f'{SCRIPT_DIR}/{DATA_SCRIPT}', 'rt') as fin:\n",
    "        with open(f'{TMP_DIR}/{TMP_SCRIPT}', 'wt') as fout:\n",
    "            for line in fin:\n",
    "                fout.write(line.replace('START_YEAR', str(year)).replace('END_YEAR', str(year + 1)))\n",
    "    data = sqlio.read_sql_query(text(open(f'{TMP_DIR}/{TMP_SCRIPT}', 'r').read()), dev_conn)\n",
    "    os.remove(f'{TMP_DIR}/{TMP_SCRIPT}')\n",
    "    data['school_start_year'] = year\n",
    "    data['school_end_year'] = year + 1\n",
    "    school_year_data.append(data)\n",
    "    shutil.rmtree(TMP_DIR)\n",
    "school_year_data = pd.concat(school_year_data, axis=0)\n",
    "\n",
    "mass_doe_data = pd.read_pickle('mass_doe_data/clean_data.pkl')\n",
    "middle_school_pop = mass_doe_data['Enrollment by Grade (2017-18)__School__6'].fillna(0) + \\\n",
    "                    mass_doe_data['Enrollment by Grade (2017-18)__School__7'].fillna(0) + \\\n",
    "                    mass_doe_data['Enrollment by Grade (2017-18)__School__8'].fillna(0)\n",
    "middle_school_pop = middle_school_pop.rename('middle_school_pop').reset_index()\n",
    "school_year_data = school_year_data.merge(middle_school_pop, how='inner', on='school_id')\n",
    "school_year_data['a/s'] = school_year_data['assignment_log_count'] / school_year_data['middle_school_pop']\n",
    "school_year_data.to_csv(f'{OUT_DIR}/school_year_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_year_data = pd.read_csv(f'{OUT_DIR}/school_year_data.csv')\n",
    "school_year_data['school_year'] = school_year_data['school_start_year'].astype(str) + '-' + school_year_data['school_end_year'].astype(str)\n",
    "start_year = school_year_data.groupby('school_id')['school_start_year'].min().rename('start_year').reset_index()\n",
    "school_year_data = school_year_data.merge(start_year, how='left', on='school_id')\n",
    "agg_data = school_year_data.groupby('school_end_year')['a/s'].apply(lambda x: (x >= 1).sum()).reset_index()\n",
    "count_data = school_year_data.groupby('school_end_year')['a/s'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,10))\n",
    "sns.lineplot(data=school_year_data, x='school_year', y='a/s', hue='school_id', legend=False, palette='rainbow')\n",
    "plt.ylim([0,10])\n",
    "plt.xlabel('School Year')\n",
    "plt.ylabel('Assignments per Student')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(13,10))\n",
    "sns.lineplot(data=school_year_data, x='school_year', y='a/s', hue='start_year', palette='rainbow', ci=None, legend=False)\n",
    "plt.xlabel('School Year')\n",
    "plt.ylabel('Assignments per Student')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(13,10))\n",
    "sns.lineplot(data=school_year_data, x='school_year', y='a/s', hue='start_year', palette='rainbow', legend=False)\n",
    "plt.xlabel('School Year')\n",
    "plt.ylabel('Assignments per Student')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(13,10))\n",
    "sns.lineplot(data=school_year_data, x='school_year', y='a/s', legend=False)\n",
    "plt.xlabel('School Year')\n",
    "plt.ylabel('Assignments per Student')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.lineplot(data=agg_data, x='school_end_year', y='a/s')\n",
    "plt.xlabel('School Year')\n",
    "plt.ylabel('Schools Using ASSISTments')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.lineplot(data=count_data, x='school_end_year', y='a/s')\n",
    "plt.xlabel('School Year')\n",
    "plt.ylabel('Schools That Used ASSISTments Even Once')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "assistments_data = pd.read_csv(f'{OUT_DIR}/school_year_data.csv', dtype='str')\n",
    "assistments_data = assistments_data.set_index('school_id')\n",
    "assistments_data = assistments_data.astype(float)\n",
    "\n",
    "#mass_doe_data = pd.read_pickle('mass_doe_data/clean_data.pkl')\n",
    "#middle_school_pop = mass_doe_data['Enrollment by Grade (2017-18)__School__6'].fillna(0) + \\\n",
    "#                    mass_doe_data['Enrollment by Grade (2017-18)__School__7'].fillna(0) + \\\n",
    "#                    mass_doe_data['Enrollment by Grade (2017-18)__School__8'].fillna(0)\n",
    "#middle_school_pop = middle_school_pop.rename('middle_school_pop').reset_index()\n",
    "\n",
    "#assistments_data = assistments_data.merge(middle_school_pop, how='inner', on='school_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2010] 1\n",
      "[2011] 6\n",
      "[2012] 19\n",
      "[2013] 16\n",
      "[2014] 9\n",
      "[2010, 2011] 7\n",
      "[2011, 2012] 25\n",
      "[2012, 2013] 32\n",
      "[2013, 2014] 21\n"
     ]
    }
   ],
   "source": [
    "for r in [[2010], [2011], [2012], [2013], [2014], [2010, 2011], [2011, 2012], [2012, 2013], [2013, 2014]]:\n",
    "    \n",
    "    window_usage = assistments_data[assistments_data['school_end_year'].isin(r)]\n",
    "    window_usage = window_usage.groupby('school_id')[['assignment_log_count', 'middle_school_pop']].mean()\n",
    "    window_usage = window_usage.reset_index()\n",
    "    window_usage['a/s'] = window_usage['assignment_log_count'] / window_usage['middle_school_pop']\n",
    "\n",
    "    pre_window = assistments_data[assistments_data['school_end_year'] < min(r)]\n",
    "    pre_window = pre_window.groupby('school_id')[['assignment_log_count', 'middle_school_pop']].mean()\n",
    "    pre_window = pre_window.reset_index()\n",
    "    pre_window['a/s'] = pre_window['assignment_log_count'] / pre_window['middle_school_pop']\n",
    "\n",
    "    window_usage = window_usage.merge(pre_window, how='left', on='school_id')\n",
    "    window_usage = window_usage.fillna(0)\n",
    "\n",
    "    window_usage = window_usage[(window_usage['a/s_x'] > 1) & (window_usage['a/s_y'] < 0.5)]\n",
    "\n",
    "    print(r, len(window_usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data for 2012 & 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assistments_data = assistments_data[assistments_data['post_2015_05_01__pre_2018_01_01__assignment_log_count'].astype(int) >= \n",
    "                                    assistments_data['middle_school_pop']]\n",
    "\n",
    "assistments_data = assistments_data[assistments_data['pre_2015_05_01__assignment_log_count'].astype(int) < \n",
    "                                    assistments_data['middle_school_pop'] / 2]\n",
    "\n",
    "# Mark the treatment schools\n",
    "mass_doe_data['treatment'] = mass_doe_data.index.isin(assistments_data['school_id'])\n",
    "mass_doe_data['weight'] = mass_doe_data['treatment'] * 2 * (len(mass_doe_data) / len(assistments_data) - 1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data for training and predicting\n",
    "ff_X = mass_doe_data.drop(columns=['Avg. Scaled Score', 'prior_performance', 'treatment', 'weight']).values\n",
    "lstm_X = np.array(mass_doe_data['prior_performance'].tolist())\n",
    "y = mass_doe_data['treatment'].values.reshape(-1, 1)\n",
    "w = mass_doe_data['weight'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(lstm_X).sum() / lstm_X.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on x axis is school year, \n",
    "# one line for each school, \n",
    "# y axis is average number of assignments started per student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on x axis is school year, \n",
    "# y axis is number of schools using assistments (more assignments than students) that year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both of those are only using schools that have 2018 MCAS tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get model stats and propensity scores for every school\n",
    "\n",
    "propensity_scores = []\n",
    "aucs = []\n",
    "r2s = []\n",
    "for train_index, test_index in StratifiedKFold(n_splits=5, shuffle=True, random_state=0).split(ff_X, y):\n",
    "    \n",
    "    # Clear session so models don't pile up\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    # Split data into training and testing splits\n",
    "    train_ff_X, test_ff_X = ff_X[train_index], ff_X[test_index]\n",
    "    train_lstm_X, test_lstm_X = lstm_X[train_index], lstm_X[test_index]\n",
    "    train_y, test_y = y[train_index], y[test_index]\n",
    "    train_w, test_w = w[train_index], w[test_index]\n",
    "    \n",
    "    # Normalize the input data based on the training data distribution\n",
    "    ff_scaler = StandardScaler().fit(train_ff_X)\n",
    "    train_ff_X = np.nan_to_num(ff_scaler.transform(train_ff_X))\n",
    "    test_ff_X = np.nan_to_num(ff_scaler.transform(test_ff_X))\n",
    "    \n",
    "    train_lstm_X_shape = train_lstm_X.shape\n",
    "    train_stacked_lstm_X = train_lstm_X.reshape(-1, train_lstm_X_shape[-1])\n",
    "    lstm_scaler = StandardScaler().fit(train_stacked_lstm_X)\n",
    "    train_lstm_X = np.nan_to_num(lstm_scaler.transform(train_stacked_lstm_X)).reshape(train_lstm_X_shape)\n",
    "    test_lstm_X_shape = test_lstm_X.shape\n",
    "    test_stacked_lstm_X = test_lstm_X.reshape(-1, test_lstm_X_shape[-1])\n",
    "    test_lstm_X = np.nan_to_num(lstm_scaler.transform(test_stacked_lstm_X)).reshape(test_lstm_X_shape)\n",
    "    \n",
    "    # Create the neural network\n",
    "    ff_input_layer = Input(shape=train_ff_X[0].shape)\n",
    "    ff_model = Dense(units=64, activation='tanh')(ff_input_layer)\n",
    "    ff_model = Dropout(rate=0.5)(ff_model)\n",
    "    \n",
    "    lstm_input_layer = Input(shape=train_lstm_X[0].shape)\n",
    "    lstm_model = Masking(mask_value=0.0)(lstm_input_layer)\n",
    "    lstm_model = LSTM(units=64, return_sequences=False, activation='tanh', dropout=0.5, recurrent_dropout=0.5)(lstm_model)\n",
    "    \n",
    "    #model = Concatenate()([ff_model, lstm_model])\n",
    "    model = ff_model\n",
    "    output_layer = Dense(units=1, activation='sigmoid')(model)\n",
    "    \n",
    "    #combined_model = Model([ff_input_layer, lstm_input_layer], output_layer)\n",
    "    combined_model = Model(ff_input_layer, output_layer)\n",
    "    combined_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    \n",
    "    # Train the neural network\n",
    "    es = [EarlyStopping(monitor='val_loss', patience=10, min_delta=0, restore_best_weights=True)]\n",
    "    combined_model.fit(x=train_ff_X, #x=[train_ff_X, train_lstm_X],\n",
    "                       y=train_y,\n",
    "                       epochs=1000,\n",
    "                       validation_split=0.25,\n",
    "                       callbacks=es,\n",
    "                       sample_weight=train_w,\n",
    "                       verbose=0)\n",
    "    \n",
    "    # Use the neural network to predict the held-out fold\n",
    "    #pred_y = combined_model.predict([test_ff_X, test_lstm_X]).flatten()\n",
    "    pred_y = combined_model.predict(test_ff_X).flatten()\n",
    "    \n",
    "    # Update propensity scores and metrics\n",
    "    propensity_scores.extend(pred_y.tolist())\n",
    "    aucs.append(roc_auc_score(test_y.flatten(), pred_y))\n",
    "    r2s.append(r2_score(test_y.flatten(), pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Findings\n",
    "\n",
    "print(f'Model AUC = {np.mean(aucs):.4g}')\n",
    "print(f'Model R\\u00b2 = {np.mean(r2s):.4g}')\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(propensity_scores, bins=20)\n",
    "plt.xlim([0,1])\n",
    "plt.xlabel('Propensity Score')\n",
    "plt.ylabel('Prediction Frequency')\n",
    "plt.title('Frequency of Propensity Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model AUC = 0.5519\n",
    "Model R² = -10.7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
