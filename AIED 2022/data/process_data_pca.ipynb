{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hickle as hkl\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "N_SPLITS = 5\n",
    "SPLIT_ON = 'class_id'\n",
    "MDL = 60\n",
    "MSL = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data\n",
    "\n",
    "remnant_actions = pd.read_csv('raw_data/all_skill_builders_with_skill_info_in_remnant/remnant_actions.csv')\n",
    "remnant_inputs = pd.read_csv('raw_data/all_skill_builders_with_skill_info_in_remnant/remnant_inputs.csv')\n",
    "remnant_targets = pd.read_csv('raw_data/all_skill_builders_with_skill_info_in_remnant/remnant_targets.csv')\n",
    "experiment_actions = pd.read_csv('raw_data/all_skill_builders_with_skill_info_in_remnant/experiment_actions.csv')\n",
    "experiment_inputs = pd.read_csv('raw_data/all_skill_builders_with_skill_info_in_remnant/experiment_inputs.csv')\n",
    "experiment_targets = pd.read_csv('raw_data/all_skill_builders_with_skill_info_in_remnant/experiment_targets.csv')\n",
    "exp_norm_map = pd.read_csv('experiment_information/exp_norm_map.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin to partition the features that will be processed\n",
    "\n",
    "action_continuous_features = ['ln_action_1_count',\n",
    "                              'ln_action_2_count',\n",
    "                              'ln_action_3_count',\n",
    "                              'ln_action_4_count',\n",
    "                              'ln_action_5_count',\n",
    "                              'ln_action_6_count',\n",
    "                              'ln_action_7_count',\n",
    "                              'ln_action_8_count',\n",
    "                              'ln_action_9_count',\n",
    "                              'ln_action_10_count',\n",
    "                              'ln_action_11_count',\n",
    "                              'ln_action_12_count',\n",
    "                              'ln_action_13_count',\n",
    "                              'ln_action_14_count',\n",
    "                              'ln_action_15_count',\n",
    "                              'ln_action_16_count',\n",
    "                              'ln_action_17_count',\n",
    "                              'ln_action_18_count',\n",
    "                              'ln_action_19_count',\n",
    "                              'ln_action_20a_count',\n",
    "                              'ln_action_20b_count',\n",
    "                              'ln_action_21_count',\n",
    "                              'ln_action_22_count',\n",
    "                              'ln_action_23_count',\n",
    "                              'ln_action_24_count',\n",
    "                              'ln_action_25_count',\n",
    "                              'ln_action_26_count',\n",
    "                              'ln_action_27_count',\n",
    "                              'ln_action_28_count',\n",
    "                              'ln_action_29_count',\n",
    "                              'ln_action_30_count',\n",
    "                              'ln_action_31_count',\n",
    "                              'ln_action_32_count',\n",
    "                              'ln_action_33_count',\n",
    "                              'ln_action_34_count',\n",
    "                              'ln_action_35_count']\n",
    "\n",
    "recurrent_categorical_features = [#'directory_1',\n",
    "                                  #'directory_2',\n",
    "                                  #'directory_3',\n",
    "                                  'is_skill_builder',\n",
    "                                  'has_due_date',\n",
    "                                  'assignment_completed']\n",
    "\n",
    "recurrent_continuous_features = ['time_since_last_assignment_start',\n",
    "                                 'session_count_raw',\n",
    "                                 'session_count_normalized',\n",
    "                                 'session_count_class_percentile',\n",
    "                                 'day_count_raw',\n",
    "                                 'day_count_normalized',\n",
    "                                 'day_count_class_percentile',\n",
    "                                 'completed_problem_count_raw',\n",
    "                                 'completed_problem_count_normalized',\n",
    "                                 'completed_problem_count_class_percentile',\n",
    "                                 'median_ln_problem_time_on_task_raw',\n",
    "                                 'median_ln_problem_time_on_task_normalized',\n",
    "                                 'median_ln_problem_time_on_task_class_percentile',\n",
    "                                 'median_ln_problem_first_response_time_raw',\n",
    "                                 'median_ln_problem_first_response_time_normalized',\n",
    "                                 'median_ln_problem_first_response_time_class_percentile',\n",
    "                                 'average_problem_attempt_count',\n",
    "                                 'average_problem_attempt_count_normalized',\n",
    "                                 'average_problem_attempt_count_class_percentile',\n",
    "                                 'average_problem_answer_first',\n",
    "                                 'average_problem_answer_first_normalized',\n",
    "                                 'average_problem_answer_first_class_percentile',\n",
    "                                 'average_problem_correctness',\n",
    "                                 'average_problem_correctness_normalized',\n",
    "                                 'average_problem_correctness_class_percentile',\n",
    "                                 'average_problem_hint_count',\n",
    "                                 'average_problem_hint_count_normalized',\n",
    "                                 'average_problem_hint_count_class_percentile',\n",
    "                                 'average_problem_answer_given',\n",
    "                                 'average_problem_answer_given_normalized',\n",
    "                                 'average_problem_answer_given_class_percentile']\n",
    "\n",
    "prior_categorical_features = ['has_due_date']\n",
    "\n",
    "prior_continuous_features = ['student_prior_assignments_started',\n",
    "                             'student_prior_assignments_percent_completed',\n",
    "                             'student_prior_median_ln_assignment_time_on_task',\n",
    "                             'student_prior_average_problems_per_assignment',\n",
    "                             'student_prior_median_ln_problem_time_on_task',\n",
    "                             'student_prior_median_ln_problem_first_response_time',\n",
    "                             'student_prior_average_problem_correctness',\n",
    "                             'student_prior_average_problem_attempt_count',\n",
    "                             'student_prior_average_answer_first',\n",
    "                             'student_prior_average_problem_hint_count',\n",
    "                             'student_skill_prior_average_problems_per_assignment',\n",
    "                             'student_skill_prior_median_ln_problem_time_on_task',\n",
    "                             'student_skill_prior_median_ln_problem_first_response_time',\n",
    "                             'student_skill_prior_average_problem_correctness',\n",
    "                             'student_skill_prior_average_problem_attempt_count',\n",
    "                             'student_skill_prior_average_answer_first',\n",
    "                             'student_skill_prior_average_problem_hint_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def process_action_input(df_list, target_times, max_day_length, scaler=None, pca=None, n_components=None):\n",
    "    if scaler is None:\n",
    "        continuous_data = np.concatenate([df[action_continuous_features].values for df in df_list])\n",
    "        scaler = MinMaxScaler().fit(continuous_data)\n",
    "        \n",
    "        continuous_data = scaler.transform(continuous_data)\n",
    "        pca = PCA().fit(continuous_data)\n",
    "        n_components = np.min(np.where(np.cumsum(pca.explained_variance_ratio_) > 0.9)) + 1\n",
    "        \n",
    "    processed_input = []\n",
    "    for df, target_time in zip(df_list, target_times):\n",
    "        end_time = target_time - (target_time % 86400)\n",
    "        start_time = end_time - max_day_length * 86400\n",
    "        timeseries = pd.DataFrame(data=np.arange(start_time, end_time, 86400), columns=['timestamp'])\n",
    "        timeseries = timeseries.merge(df, how='left', on='timestamp')\n",
    "        timeseries = timeseries[action_continuous_features].fillna(0).values\n",
    "        timeseries = scaler.transform(timeseries)\n",
    "        timeseries = pca.transform(timeseries)[:,:n_components]\n",
    "        processed_input.append(timeseries)\n",
    "    return np.stack(processed_input), scaler, pca, n_components\n",
    "\n",
    "\n",
    "def process_recurrent_input(df_list, max_sequence_length, one_hot_encoder=None, normalizer=None, pca=None, n_components=None):\n",
    "    if one_hot_encoder is None:\n",
    "        categorical_data = np.concatenate([df[recurrent_categorical_features].values for df in df_list])\n",
    "        continuous_data = np.concatenate([df[recurrent_continuous_features].values for df in df_list])\n",
    "        one_hot_encoder = OneHotEncoder(handle_unknown='ignore').fit(categorical_data)\n",
    "        normalizer = StandardScaler().fit(continuous_data)\n",
    "        \n",
    "        categorical_data = one_hot_encoder.transform(categorical_data).toarray()\n",
    "        continuous_data = np.nan_to_num(normalizer.transform(continuous_data))\n",
    "        combined_data = np.concatenate([categorical_data, continuous_data], axis=1)\n",
    "        pca = PCA().fit(combined_data)\n",
    "        n_components = np.min(np.where(np.cumsum(pca.explained_variance_ratio_) > 0.9)) + 1\n",
    "    \n",
    "    processed_input = []\n",
    "    for df in df_list:\n",
    "        categorical_data = one_hot_encoder.transform(df[recurrent_categorical_features]).toarray()\n",
    "        continuous_data = np.nan_to_num(normalizer.transform(df[recurrent_continuous_features]))\n",
    "        combined_data = np.concatenate([categorical_data, continuous_data], axis=1)\n",
    "        if combined_data.shape[0] >= max_sequence_length:\n",
    "            resized_data = combined_data[-max_sequence_length:, :]\n",
    "        else:\n",
    "            resized_data = np.zeros((max_sequence_length, combined_data.shape[1]))\n",
    "            resized_data[-combined_data.shape[0]:, :] = combined_data\n",
    "        pca.transform(resized_data)[:,:n_components]\n",
    "        processed_input.append(resized_data)\n",
    "    return np.stack(processed_input), one_hot_encoder, normalizer, pca, n_components\n",
    "\n",
    "\n",
    "def process_prior_input(df, one_hot_encoder=None, normalizer=None, pca=None, n_components=None):\n",
    "    if one_hot_encoder is None:\n",
    "        categorical_data = df[prior_categorical_features].values\n",
    "        continuous_data = df[prior_continuous_features].values\n",
    "        one_hot_encoder = OneHotEncoder(handle_unknown='ignore').fit(categorical_data)\n",
    "        normalizer = StandardScaler().fit(continuous_data)\n",
    "        \n",
    "        categorical_data = one_hot_encoder.transform(categorical_data).toarray()\n",
    "        continuous_data = np.nan_to_num(normalizer.transform(continuous_data))\n",
    "        combined_data = np.concatenate([categorical_data, continuous_data], axis=1)\n",
    "        pca = PCA().fit(combined_data)\n",
    "        n_components = np.min(np.where(np.cumsum(pca.explained_variance_ratio_) > 0.9)) + 1\n",
    "        \n",
    "    categorical_data = one_hot_encoder.transform(df[prior_categorical_features]).toarray()\n",
    "    continuous_data = np.nan_to_num(normalizer.transform(df[prior_continuous_features]))\n",
    "    combined_data = np.concatenate([categorical_data, continuous_data], axis=1)\n",
    "    return combined_data, one_hot_encoder, normalizer, pca, n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features and target variables\n",
    "\n",
    "# Create variable for the corresponding experiment sequence ID\n",
    "exp_norm = exp_norm_map.dropna()\n",
    "norm_exp_dict = dict(zip(exp_norm['normal_id'], exp_norm['experiment_id']))\n",
    "exp_exp_dict = dict(zip(exp_norm['experiment_id'], exp_norm['experiment_id']))\n",
    "remnant_targets['shared_sequence_id'] = remnant_targets['target_sequence'].map(norm_exp_dict).fillna('None')\n",
    "experiment_targets['shared_sequence_id'] = experiment_targets['target_sequence'].map(exp_exp_dict).fillna('None')\n",
    "prior_categorical_features.append('shared_sequence_id')\n",
    "\n",
    "# An explicit feature for which cluster the time_since_last_assignment_start falls into\n",
    "remnant_times = remnant_inputs['time_since_last_assignment_start'].values.reshape(-1, 1)\n",
    "experiment_times = experiment_inputs['time_since_last_assignment_start'].values.reshape(-1, 1)\n",
    "gmm = GaussianMixture(n_components=4).fit(remnant_times)\n",
    "remnant_inputs['time_since_last_assignment_start_cluster'] = gmm.predict(remnant_times)\n",
    "experiment_inputs['time_since_last_assignment_start_cluster'] = gmm.predict(experiment_times)\n",
    "recurrent_categorical_features.append('time_since_last_assignment_start_cluster')\n",
    "\n",
    "# A feature for whether or not there is a folder path\n",
    "remnant_inputs['custom_assignment'] = remnant_inputs['directory_1'].isna().astype(int)\n",
    "experiment_inputs['custom_assignment'] = experiment_inputs['directory_1'].isna().astype(int)\n",
    "recurrent_categorical_features.append('custom_assignment')\n",
    "\n",
    "# A feature for whether or not there is any problem level data\n",
    "remnant_inputs['no_problem_statistics'] = remnant_inputs['median_ln_problem_time_on_task_raw'].isna().astype(int)\n",
    "experiment_inputs['no_problem_statistics'] = experiment_inputs['median_ln_problem_time_on_task_raw'].isna().astype(int)\n",
    "recurrent_categorical_features.append('no_problem_statistics')\n",
    "\n",
    "# feature for whether or not the student did problems of the same skill previously\n",
    "remnant_targets['no_skill_priors'] = remnant_targets['student_skill_prior_average_problems_per_assignment'].isna().astype(int)\n",
    "experiment_targets['no_skill_priors'] = experiment_targets['student_skill_prior_average_problems_per_assignment'].isna().astype(int)\n",
    "prior_categorical_features.append('no_skill_priors')\n",
    "\n",
    "# Features for the year and month of the assignments\n",
    "remnant_inputs['year'] = pd.DatetimeIndex(pd.to_datetime(remnant_inputs['assignment_start_time'], unit='s')).year\n",
    "remnant_inputs['month'] = pd.DatetimeIndex(pd.to_datetime(remnant_inputs['assignment_start_time'], unit='s')).month\n",
    "experiment_inputs['year'] = pd.DatetimeIndex(pd.to_datetime(experiment_inputs['assignment_start_time'], unit='s')).year\n",
    "experiment_inputs['month'] = pd.DatetimeIndex(pd.to_datetime(experiment_inputs['assignment_start_time'], unit='s')).month\n",
    "recurrent_categorical_features.append('year')\n",
    "recurrent_categorical_features.append('month')\n",
    "\n",
    "remnant_targets['year'] = pd.DatetimeIndex(pd.to_datetime(remnant_targets['assignment_start_time'], unit='s')).year\n",
    "remnant_targets['month'] = pd.DatetimeIndex(pd.to_datetime(remnant_targets['assignment_start_time'], unit='s')).month\n",
    "experiment_targets['year'] = pd.DatetimeIndex(pd.to_datetime(experiment_targets['assignment_start_time'], unit='s')).year\n",
    "experiment_targets['month'] = pd.DatetimeIndex(pd.to_datetime(experiment_targets['assignment_start_time'], unit='s')).month\n",
    "prior_categorical_features.append('year')\n",
    "prior_categorical_features.append('month')\n",
    "\n",
    "# Replace NaN categorical values with -1\n",
    "remnant_inputs[recurrent_categorical_features] = remnant_inputs[recurrent_categorical_features].fillna(-1)\n",
    "remnant_targets[prior_categorical_features] = remnant_targets[prior_categorical_features].fillna(-1)\n",
    "experiment_inputs[recurrent_categorical_features] = experiment_inputs[recurrent_categorical_features].fillna(-1)\n",
    "experiment_targets[prior_categorical_features] = experiment_targets[prior_categorical_features].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the previous assignments for each sample\n",
    "\n",
    "remnant_targets['previous_assignments'] = None\n",
    "remnant_targets['previous_actions'] = None\n",
    "for i, row in remnant_targets.iterrows():\n",
    "    past_assignment = remnant_actions['timestamp'] + 86400 < row['assignment_start_time']\n",
    "    same_student = remnant_actions['student_id'] == row['student_id']\n",
    "    row_actions = remnant_actions[past_assignment & same_student].sort_values('timestamp')\n",
    "    remnant_targets.at[i, 'previous_actions'] = row_actions if len(row_actions) > 0 else None\n",
    "    \n",
    "    past_assignment = remnant_inputs['assignment_start_time'] < row['assignment_start_time']\n",
    "    same_student = remnant_inputs['student_id'] == row['student_id']\n",
    "    row_input = remnant_inputs[past_assignment & same_student].sort_values('assignment_start_time')\n",
    "    remnant_targets.at[i, 'previous_assignments'] = row_input if len(row_input) > 0 else None\n",
    "\n",
    "remnant_targets = remnant_targets.dropna(subset=['previous_actions', 'previous_assignments'])\n",
    "\n",
    "\n",
    "experiment_targets['previous_assignments'] = None\n",
    "experiment_targets['previous_actions'] = None\n",
    "for i, row in experiment_targets.iterrows():\n",
    "    past_assignment = experiment_actions['timestamp'] + 86400 < row['assignment_start_time']\n",
    "    same_student = experiment_actions['student_id'] == row['student_id']\n",
    "    row_actions = experiment_actions[past_assignment & same_student].sort_values('timestamp')\n",
    "    experiment_targets.at[i, 'previous_actions'] = row_actions if len(row_actions) > 0 else None\n",
    "    \n",
    "    past_assignment = experiment_inputs['assignment_start_time'] < row['assignment_start_time']\n",
    "    same_student = experiment_inputs['student_id'] == row['student_id']\n",
    "    row_input = experiment_inputs[past_assignment & same_student].sort_values('assignment_start_time')\n",
    "    experiment_targets.at[i, 'previous_assignments'] = row_input if len(row_input) > 0 else None\n",
    "\n",
    "experiment_targets = experiment_targets.dropna(subset=['previous_actions', 'previous_assignments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets for validating the model\n",
    "\n",
    "outdir = f'processed_data/pca_cv_data__{N_SPLITS}_{SPLIT_ON}_folds__{MSL}_steps__{MDL}_days'\n",
    "os.mkdir(outdir)\n",
    "\n",
    "count = 0\n",
    "for train_index, test_index in GroupKFold(N_SPLITS).split(remnant_targets, groups=remnant_targets[SPLIT_ON]):\n",
    "\n",
    "    action_training_input = remnant_targets.iloc[train_index]['previous_actions'].tolist()\n",
    "    target_times = remnant_targets.iloc[train_index]['assignment_start_time'].tolist()\n",
    "    action_training_content = process_action_input(action_training_input, target_times, MDL)\n",
    "    action_training_input, action_scaler, action_pca, action_n_components = action_training_content\n",
    "    \n",
    "    recurrent_training_input = remnant_targets.iloc[train_index]['previous_assignments'].tolist()\n",
    "    recurrent_training_content = process_recurrent_input(recurrent_training_input, MSL)\n",
    "    recurrent_training_input, recurrent_encoder, recurrent_normalizer, recurrent_pca, recurrent_n_components = recurrent_training_content\n",
    "    \n",
    "    prior_training_input = remnant_targets.iloc[train_index]\n",
    "    prior_training_content = process_prior_input(prior_training_input)\n",
    "    prior_training_input, prior_encoder, prior_normalizer, prior_pca, prior_n_components = prior_training_content\n",
    "    \n",
    "    completion_training_target = remnant_targets.iloc[train_index][['assignment_completed']].values\n",
    "    \n",
    "    problems_training_target = remnant_targets.iloc[train_index][['problems_completed']].values\n",
    "    \n",
    "    training_target_sequence = remnant_targets.iloc[train_index][['target_sequence']].values\n",
    "    \n",
    "    action_testing_input = remnant_targets.iloc[test_index]['previous_actions'].tolist()\n",
    "    target_times = remnant_targets.iloc[test_index]['assignment_start_time'].tolist()\n",
    "    action_testing_input, _, _, _ = process_action_input(action_testing_input, target_times, MDL, action_scaler, action_pca, action_n_components)\n",
    "    \n",
    "    recurrent_testing_input = remnant_targets.iloc[test_index]['previous_assignments'].tolist()\n",
    "    recurrent_testing_input, _, _, _, _ = process_recurrent_input(recurrent_testing_input, MSL, recurrent_encoder, recurrent_normalizer, recurrent_pca, recurrent_n_components)\n",
    "    \n",
    "    prior_testing_input = remnant_targets.iloc[test_index]\n",
    "    prior_testing_input, _, _, _, _ = process_prior_input(prior_testing_input, prior_encoder, prior_normalizer, prior_pca, prior_n_components)\n",
    "    \n",
    "    completion_testing_target = remnant_targets.iloc[test_index][['assignment_completed']].values\n",
    "    \n",
    "    problems_testing_target = remnant_targets.iloc[test_index][['problems_completed']].values\n",
    "    \n",
    "    testing_target_sequence = remnant_targets.iloc[test_index][['target_sequence']].values\n",
    "    \n",
    "    data = {'action_training_input': action_training_input, \n",
    "            'recurrent_training_input': recurrent_training_input,\n",
    "            'prior_training_input': prior_training_input,\n",
    "            'completion_training_target': completion_training_target,\n",
    "            'problems_training_target': problems_training_target,\n",
    "            'training_target_sequence': training_target_sequence,\n",
    "            'action_testing_input': action_testing_input, \n",
    "            'recurrent_testing_input': recurrent_testing_input,\n",
    "            'prior_testing_input': prior_testing_input,\n",
    "            'completion_testing_target': completion_testing_target,\n",
    "            'problems_testing_target': problems_testing_target,\n",
    "            'testing_target_sequence': testing_target_sequence}\n",
    "    hkl.dump(data, f'{outdir}/fold_{count}.hkl', mode='w')\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets for validating the model\n",
    "\n",
    "action_remnant_input = remnant_targets['previous_actions'].tolist()\n",
    "target_times = remnant_targets['assignment_start_time'].tolist()\n",
    "action_remnant_content = process_action_input(action_remnant_input, target_times, MDL)\n",
    "action_remnant_input, action_scaler, action_pca, action_n_components = action_remnant_content\n",
    "\n",
    "recurrent_remnant_input = remnant_targets['previous_assignments'].tolist()\n",
    "recurrent_remnant_content = process_recurrent_input(recurrent_remnant_input, MSL)\n",
    "recurrent_remnant_input, recurrent_encoder, recurrent_normalizer, recurrent_pca, recurrent_n_components = recurrent_remnant_content\n",
    "\n",
    "prior_remnant_input = remnant_targets\n",
    "prior_remnant_content = process_prior_input(prior_remnant_input)\n",
    "prior_remnant_input, prior_encoder, prior_normalizer, prior_pca, prior_n_components = prior_remnant_content\n",
    "\n",
    "completion_remnant_target = remnant_targets[['assignment_completed']].values\n",
    "\n",
    "problems_remnant_target = remnant_targets[['problems_completed']].values\n",
    "\n",
    "remnant_sequence = remnant_targets[['target_sequence']].values\n",
    "\n",
    "action_experiment_input = experiment_targets['previous_actions'].tolist()\n",
    "target_times = experiment_targets['assignment_start_time'].tolist()\n",
    "action_experiment_input, _, _, _ = process_action_input(action_experiment_input, target_times, MDL, action_scaler, action_pca, action_n_components)\n",
    "\n",
    "recurrent_experiment_input = experiment_targets['previous_assignments'].tolist()\n",
    "recurrent_experiment_input, _, _, _, _ = process_recurrent_input(recurrent_experiment_input, MSL, recurrent_encoder, recurrent_normalizer, recurrent_pca, recurrent_n_components)\n",
    "\n",
    "prior_experiment_input = experiment_targets\n",
    "prior_experiment_input, _, _, _, _ = process_prior_input(prior_experiment_input, prior_encoder, prior_normalizer, prior_pca, prior_n_components)\n",
    "\n",
    "completion_experiment_target = experiment_targets[['assignment_completed']].values\n",
    "\n",
    "problems_experiment_target = experiment_targets[['problems_completed']].values\n",
    "\n",
    "experiment_sequence = experiment_targets[['target_sequence']].values\n",
    "\n",
    "data = {'action_remnant_input': action_remnant_input, \n",
    "        'recurrent_remnant_input': recurrent_remnant_input,\n",
    "        'prior_remnant_input': prior_remnant_input,\n",
    "        'completion_remnant_target': completion_remnant_target,\n",
    "        'problems_remnant_target': problems_remnant_target,\n",
    "        'remnant_sequence': remnant_sequence,\n",
    "        'action_experiment_input': action_experiment_input, \n",
    "        'recurrent_experiment_input': recurrent_experiment_input,\n",
    "        'prior_experiment_input': prior_experiment_input,\n",
    "        'completion_experiment_target': completion_experiment_target,\n",
    "        'problems_experiment_target': problems_experiment_target,\n",
    "        'experiment_sequence': experiment_sequence}\n",
    "\n",
    "hkl.dump(data, f'processed_data/pca_experiment_data__{MSL}_steps__{MDL}_days.hkl', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 7\n",
      "61 22\n",
      "63 17\n"
     ]
    }
   ],
   "source": [
    "print(len(action_pca.explained_variance_ratio_), action_n_components)\n",
    "print(len(recurrent_pca.explained_variance_ratio_), recurrent_n_components)\n",
    "print(len(prior_pca.explained_variance_ratio_), prior_n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
